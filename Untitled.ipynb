{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as plt\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "config = tf.ConfigProto( device_count = {'GPU' : 1, 'CPU':16})\n",
    "sess = tf.Session(config=config)\n",
    "keras.backend.set_session(sess)\n",
    "import random\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.applications import *\n",
    "from keras.optimizers import *\n",
    "from keras.regularizers import *\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.utils import plot_model\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import signal\n",
    "from keras.preprocessing import sequence\n",
    "from keras.regularizers import l2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tongue_left': 0, 'lips_pipe': 1, 'tongue_up': 2, 'lips_smile_teeth': 3, 'tongue_down': 4, 'tongue_right': 5}\n"
     ]
    }
   ],
   "source": [
    "cascade = cv2.CascadeClassifier('./weights/haarcascade_frontalface_alt.xml')\n",
    "size = 224\n",
    "\n",
    "def get_facial_landmark_model(path):\n",
    "    model = load_model(path)\n",
    "    return model\n",
    "\n",
    "def get_seq_model():\n",
    "    model =  Sequential()\n",
    "    model.add(((LSTM(256,dropout=0.2,input_shape=(200,71), return_sequences = True, stateful = False))))\n",
    "    model.add((LSTM(512,dropout=0.2,return_sequences = False )))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(100))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(6))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def get_video_features(stream,model):\n",
    "    X = []\n",
    "    while True:\n",
    "        _,img = stream.read()\n",
    "        if (_== False):\n",
    "            break\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        if (len(faces) == 0):\n",
    "            continue\n",
    "        #выделение лица\n",
    "        x,y,w,h = faces[0]\n",
    "        img = img[y:y+h+10, x:x+w+10]\n",
    "        img = cv2.resize(img,(size,size))\n",
    "        blob = np.expand_dims(img,axis=0)\n",
    "        #тепловая карта лэндмарок на лице\n",
    "        #тут можно пробовать другие модели/автокодировщик/или нормальный facial landmark но я наткой не нашёл\n",
    "        pred  = model.predict(blob)\n",
    "        pred = np.transpose(pred, (0,3,1,2))\n",
    "        features = []\n",
    "        for i in range(71):\n",
    "            hetMap = pred[0,i,:,:]\n",
    "            _, conf, _, point = cv2.minMaxLoc(hetMap)\n",
    "            x = (size * point[0]) / pred.shape[3]\n",
    "            y = (size * point[1]) / pred.shape[2]\n",
    "            point = [x/size,y/size,conf]\n",
    "            features.append(conf)\n",
    "        features = np.array(features)\n",
    "        X.append(features)\n",
    "    return X\n",
    "\n",
    "cls_names = os.listdir('./tongue-detection/')\n",
    "class_ids = {cls_names[i]:i for i in range(len(cls_names))}\n",
    "print(class_ids)\n",
    "\n",
    "def get_DATA(path_to_folder,model):\n",
    "    X = []\n",
    "    y = []\n",
    "    for sign in os.listdir(path_to_folder):\n",
    "        for file in os.listdir(os.path.join(path_to_folder,sign)):\n",
    "            vs = cv2.VideoCapture('{}/{}/{}'.format(path_to_folder,sign,file))\n",
    "            vs_features = get_video_features(vs,model)\n",
    "            if (len(vs_features) == 0):\n",
    "                print ('skip')\n",
    "                continue\n",
    "            X.append(vs_features)\n",
    "            y.append(class_ids[sign])\n",
    "    return X,y\n",
    "\n",
    "def save_as_h5(name, features,labels):\n",
    "    with h5py.File(name, \"w\") as f:\n",
    "        f.create_dataset(\"features\", data=features)\n",
    "        f.create_dataset(\"labels\", data=labels)\n",
    "        \n",
    "def get_saved(path):\n",
    "    val = h5py.File(path, 'r')\n",
    "    val_features = val[\"features\"][: ]\n",
    "    val_labels = val[\"labels\"][: ]\n",
    "    return val_features, val_labels\n",
    "\n",
    "def plot(history):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.savefig('acc.png')\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.savefig('loss.png')\n",
    "    \n",
    "def scheduler(epoch):\n",
    "    if epoch < 40:\n",
    "        return (.001)\n",
    "    elif epoch < 80:\n",
    "        return (.0001)\n",
    "    else:\n",
    "        return (.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_facial_landmark_model('./weights/faces.h5')\n",
    "X,Y = get_DATA('./tongue-detection/', model)\n",
    "train_features,test_features,train_labels,test_labels = train_test_split(features, labels, test_size=0.1, random_state=42)\n",
    "fit_features = sequence.pad_sequences(train_features,maxlen=200, dtype='float32')\n",
    "predict_features = sequence.pad_sequences(test_features, maxlen=200,dtype='float32') \n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "save_as_h5('features_train.h5',fit_features,train_labels)\n",
    "save_as_h5('features_test.h5',predict_features,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_features,train_labels = get_save('features_train.h5')\n",
    "predict_features,test_labels = get_save('features_test.h5')\n",
    "seq_m = get_seq_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 200, 256)          335872    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 512)               1574912   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               51300     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 606       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 1,962,690\n",
      "Trainable params: 1,962,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 608 samples, validate on 68 samples\n",
      "Epoch 1/160\n",
      "608/608 [==============================] - 15s 25ms/step - loss: 1.3849 - acc: 0.3914 - val_loss: 1.4790 - val_acc: 0.3235\n",
      "Epoch 2/160\n",
      "608/608 [==============================] - 14s 24ms/step - loss: 1.4811 - acc: 0.3684 - val_loss: 1.4823 - val_acc: 0.3529\n",
      "Epoch 3/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 1.4249 - acc: 0.3898 - val_loss: 1.4548 - val_acc: 0.2647\n",
      "Epoch 4/160\n",
      "608/608 [==============================] - 17s 28ms/step - loss: 1.3439 - acc: 0.4309 - val_loss: 1.4989 - val_acc: 0.3088\n",
      "Epoch 5/160\n",
      "608/608 [==============================] - 17s 28ms/step - loss: 1.3284 - acc: 0.4063 - val_loss: 1.5426 - val_acc: 0.3088\n",
      "Epoch 6/160\n",
      "608/608 [==============================] - 17s 27ms/step - loss: 1.3607 - acc: 0.4309 - val_loss: 1.4321 - val_acc: 0.3235\n",
      "Epoch 7/160\n",
      "608/608 [==============================] - 14s 24ms/step - loss: 1.3215 - acc: 0.4309 - val_loss: 1.4243 - val_acc: 0.3235\n",
      "Epoch 8/160\n",
      "608/608 [==============================] - 14s 24ms/step - loss: 1.3546 - acc: 0.4013 - val_loss: 1.4551 - val_acc: 0.3529\n",
      "Epoch 9/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 1.3358 - acc: 0.4408 - val_loss: 1.4167 - val_acc: 0.2794\n",
      "Epoch 10/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 1.3168 - acc: 0.4441 - val_loss: 1.4963 - val_acc: 0.2941\n",
      "Epoch 11/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 1.3304 - acc: 0.4408 - val_loss: 1.4297 - val_acc: 0.3088\n",
      "Epoch 12/160\n",
      "608/608 [==============================] - 14s 24ms/step - loss: 1.3306 - acc: 0.4293 - val_loss: 1.4483 - val_acc: 0.3382\n",
      "Epoch 13/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 1.3588 - acc: 0.4276 - val_loss: 1.5529 - val_acc: 0.2941\n",
      "Epoch 14/160\n",
      "608/608 [==============================] - 18s 29ms/step - loss: 1.3212 - acc: 0.4490 - val_loss: 1.5194 - val_acc: 0.3529\n",
      "Epoch 15/160\n",
      "608/608 [==============================] - 18s 29ms/step - loss: 1.2880 - acc: 0.4391 - val_loss: 1.4068 - val_acc: 0.3382\n",
      "Epoch 16/160\n",
      "608/608 [==============================] - 19s 31ms/step - loss: 1.3232 - acc: 0.4211 - val_loss: 1.4378 - val_acc: 0.3529\n",
      "Epoch 17/160\n",
      "608/608 [==============================] - 17s 28ms/step - loss: 1.3370 - acc: 0.4260 - val_loss: 1.4567 - val_acc: 0.3088\n",
      "Epoch 18/160\n",
      "608/608 [==============================] - 18s 30ms/step - loss: 1.2605 - acc: 0.4622 - val_loss: 1.6946 - val_acc: 0.3088\n",
      "Epoch 19/160\n",
      "608/608 [==============================] - 14s 24ms/step - loss: 1.3363 - acc: 0.3964 - val_loss: 1.5652 - val_acc: 0.2794\n",
      "Epoch 20/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 1.2984 - acc: 0.4556 - val_loss: 1.4538 - val_acc: 0.2353\n",
      "Epoch 21/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 1.2324 - acc: 0.4720 - val_loss: 1.5075 - val_acc: 0.3676\n",
      "Epoch 22/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 1.2856 - acc: 0.4490 - val_loss: 1.4392 - val_acc: 0.3676\n",
      "Epoch 23/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 1.2383 - acc: 0.4622 - val_loss: 1.4865 - val_acc: 0.3088\n",
      "Epoch 24/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 1.2135 - acc: 0.4901 - val_loss: 1.6322 - val_acc: 0.2941\n",
      "Epoch 25/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 1.2503 - acc: 0.4720 - val_loss: 1.4546 - val_acc: 0.2647\n",
      "Epoch 26/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 1.1909 - acc: 0.4671 - val_loss: 1.6480 - val_acc: 0.3088\n",
      "Epoch 27/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 1.1884 - acc: 0.4671 - val_loss: 1.6170 - val_acc: 0.2794\n",
      "Epoch 28/160\n",
      "608/608 [==============================] - 14s 24ms/step - loss: 1.1858 - acc: 0.4836 - val_loss: 1.5129 - val_acc: 0.3529\n",
      "Epoch 29/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 1.1764 - acc: 0.4868 - val_loss: 1.6476 - val_acc: 0.3382\n",
      "Epoch 30/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 1.1693 - acc: 0.4918 - val_loss: 1.4986 - val_acc: 0.3235\n",
      "Epoch 31/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 1.1988 - acc: 0.4836 - val_loss: 1.6130 - val_acc: 0.2941\n",
      "Epoch 32/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 1.2044 - acc: 0.4704 - val_loss: 1.4571 - val_acc: 0.2941\n",
      "Epoch 33/160\n",
      "608/608 [==============================] - 14s 24ms/step - loss: 1.1713 - acc: 0.4704 - val_loss: 1.5702 - val_acc: 0.3235\n",
      "Epoch 34/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 1.1571 - acc: 0.4885 - val_loss: 1.6629 - val_acc: 0.2794\n",
      "Epoch 35/160\n",
      "608/608 [==============================] - 14s 24ms/step - loss: 1.2848 - acc: 0.4622 - val_loss: 1.6090 - val_acc: 0.3529\n",
      "Epoch 36/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 1.4194 - acc: 0.4227 - val_loss: 1.8366 - val_acc: 0.2647\n",
      "Epoch 37/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 1.3235 - acc: 0.4655 - val_loss: 1.5597 - val_acc: 0.3235\n",
      "Epoch 38/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 1.3180 - acc: 0.4474 - val_loss: 1.5384 - val_acc: 0.3971\n",
      "Epoch 39/160\n",
      "608/608 [==============================] - 13s 21ms/step - loss: 1.2599 - acc: 0.4523 - val_loss: 1.6154 - val_acc: 0.2794\n",
      "Epoch 40/160\n",
      "608/608 [==============================] - 13s 21ms/step - loss: 1.2401 - acc: 0.4967 - val_loss: 1.5716 - val_acc: 0.2941\n",
      "Epoch 41/160\n",
      "608/608 [==============================] - 13s 21ms/step - loss: 1.1650 - acc: 0.4951 - val_loss: 1.5682 - val_acc: 0.3529\n",
      "Epoch 42/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 1.1754 - acc: 0.5181 - val_loss: 1.5511 - val_acc: 0.3235\n",
      "Epoch 43/160\n",
      "608/608 [==============================] - 13s 22ms/step - loss: 1.1395 - acc: 0.5000 - val_loss: 1.5898 - val_acc: 0.2941\n",
      "Epoch 44/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 1.0808 - acc: 0.5510 - val_loss: 1.5667 - val_acc: 0.2647\n",
      "Epoch 45/160\n",
      "608/608 [==============================] - 15s 25ms/step - loss: 1.1146 - acc: 0.5000 - val_loss: 1.5745 - val_acc: 0.3088\n",
      "Epoch 46/160\n",
      "608/608 [==============================] - 15s 24ms/step - loss: 1.0844 - acc: 0.5164 - val_loss: 1.5849 - val_acc: 0.3382\n",
      "Epoch 47/160\n",
      "608/608 [==============================] - 15s 24ms/step - loss: 1.0743 - acc: 0.5313 - val_loss: 1.5836 - val_acc: 0.3235\n",
      "Epoch 48/160\n",
      "608/608 [==============================] - 15s 25ms/step - loss: 1.1118 - acc: 0.5197 - val_loss: 1.5695 - val_acc: 0.2941\n",
      "Epoch 49/160\n",
      "608/608 [==============================] - 15s 24ms/step - loss: 1.0969 - acc: 0.5148 - val_loss: 1.5845 - val_acc: 0.3235\n",
      "Epoch 50/160\n",
      "608/608 [==============================] - 13s 22ms/step - loss: 1.0735 - acc: 0.5428 - val_loss: 1.5920 - val_acc: 0.2941\n",
      "Epoch 51/160\n",
      "608/608 [==============================] - 14s 24ms/step - loss: 1.1136 - acc: 0.4934 - val_loss: 1.6386 - val_acc: 0.3235\n",
      "Epoch 52/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 1.0822 - acc: 0.5230 - val_loss: 1.5980 - val_acc: 0.3088\n",
      "Epoch 53/160\n",
      "608/608 [==============================] - 15s 24ms/step - loss: 1.0668 - acc: 0.5132 - val_loss: 1.5940 - val_acc: 0.3235\n",
      "Epoch 54/160\n",
      "608/608 [==============================] - 15s 25ms/step - loss: 1.0472 - acc: 0.5625 - val_loss: 1.5398 - val_acc: 0.3529\n",
      "Epoch 55/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 1.0550 - acc: 0.5329 - val_loss: 1.5847 - val_acc: 0.3235\n",
      "Epoch 56/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 1.0478 - acc: 0.5757 - val_loss: 1.5679 - val_acc: 0.3529\n",
      "Epoch 57/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 1.0728 - acc: 0.5280 - val_loss: 1.5949 - val_acc: 0.3088\n",
      "Epoch 58/160\n",
      "608/608 [==============================] - 16s 26ms/step - loss: 1.1030 - acc: 0.5263 - val_loss: 1.5747 - val_acc: 0.3235\n",
      "Epoch 59/160\n",
      "608/608 [==============================] - 17s 28ms/step - loss: 1.0548 - acc: 0.5263 - val_loss: 1.6134 - val_acc: 0.3088\n",
      "Epoch 60/160\n",
      "608/608 [==============================] - 15s 24ms/step - loss: 1.0383 - acc: 0.5329 - val_loss: 1.6083 - val_acc: 0.3088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/160\n",
      "608/608 [==============================] - 13s 21ms/step - loss: 1.0150 - acc: 0.5461 - val_loss: 1.5940 - val_acc: 0.3382\n",
      "Epoch 62/160\n",
      "608/608 [==============================] - 13s 21ms/step - loss: 1.0272 - acc: 0.5510 - val_loss: 1.5926 - val_acc: 0.3382\n",
      "Epoch 63/160\n",
      "608/608 [==============================] - 13s 22ms/step - loss: 1.0224 - acc: 0.5477 - val_loss: 1.5837 - val_acc: 0.3824\n",
      "Epoch 64/160\n",
      "608/608 [==============================] - 16s 26ms/step - loss: 1.0524 - acc: 0.5526 - val_loss: 1.6003 - val_acc: 0.3676\n",
      "Epoch 65/160\n",
      "608/608 [==============================] - 13s 22ms/step - loss: 1.0260 - acc: 0.5510 - val_loss: 1.5607 - val_acc: 0.3529\n",
      "Epoch 66/160\n",
      "608/608 [==============================] - 13s 22ms/step - loss: 1.0195 - acc: 0.5444 - val_loss: 1.5865 - val_acc: 0.3971\n",
      "Epoch 67/160\n",
      "608/608 [==============================] - 15s 25ms/step - loss: 1.0296 - acc: 0.5493 - val_loss: 1.6147 - val_acc: 0.3824\n",
      "Epoch 68/160\n",
      "608/608 [==============================] - 16s 27ms/step - loss: 1.0349 - acc: 0.5724 - val_loss: 1.6032 - val_acc: 0.3824\n",
      "Epoch 69/160\n",
      "608/608 [==============================] - 17s 28ms/step - loss: 1.0338 - acc: 0.5493 - val_loss: 1.6385 - val_acc: 0.3529\n",
      "Epoch 70/160\n",
      "608/608 [==============================] - 17s 28ms/step - loss: 1.0382 - acc: 0.5296 - val_loss: 1.5965 - val_acc: 0.3382\n",
      "Epoch 71/160\n",
      "608/608 [==============================] - 17s 28ms/step - loss: 1.0430 - acc: 0.5477 - val_loss: 1.6082 - val_acc: 0.3971\n",
      "Epoch 72/160\n",
      "608/608 [==============================] - 16s 26ms/step - loss: 0.9874 - acc: 0.5789 - val_loss: 1.6613 - val_acc: 0.3676\n",
      "Epoch 73/160\n",
      "608/608 [==============================] - 16s 27ms/step - loss: 1.0234 - acc: 0.5625 - val_loss: 1.6404 - val_acc: 0.3235\n",
      "Epoch 74/160\n",
      "608/608 [==============================] - 17s 27ms/step - loss: 1.0487 - acc: 0.5428 - val_loss: 1.6610 - val_acc: 0.3824\n",
      "Epoch 75/160\n",
      "608/608 [==============================] - 16s 27ms/step - loss: 1.0173 - acc: 0.5493 - val_loss: 1.6057 - val_acc: 0.3676\n",
      "Epoch 76/160\n",
      "608/608 [==============================] - 17s 27ms/step - loss: 1.0132 - acc: 0.5707 - val_loss: 1.6903 - val_acc: 0.3382\n",
      "Epoch 77/160\n",
      "608/608 [==============================] - 16s 27ms/step - loss: 0.9774 - acc: 0.5609 - val_loss: 1.6481 - val_acc: 0.3676\n",
      "Epoch 78/160\n",
      "608/608 [==============================] - 16s 27ms/step - loss: 0.9872 - acc: 0.5411 - val_loss: 1.5873 - val_acc: 0.3824\n",
      "Epoch 79/160\n",
      "608/608 [==============================] - 16s 26ms/step - loss: 1.0274 - acc: 0.5510 - val_loss: 1.6348 - val_acc: 0.3529\n",
      "Epoch 80/160\n",
      "608/608 [==============================] - 16s 26ms/step - loss: 0.9840 - acc: 0.5789 - val_loss: 1.6054 - val_acc: 0.3676\n",
      "Epoch 81/160\n",
      "608/608 [==============================] - 16s 26ms/step - loss: 1.0062 - acc: 0.5658 - val_loss: 1.6092 - val_acc: 0.3676\n",
      "Epoch 82/160\n",
      "608/608 [==============================] - 16s 26ms/step - loss: 0.9999 - acc: 0.5428 - val_loss: 1.6203 - val_acc: 0.3824\n",
      "Epoch 83/160\n",
      "608/608 [==============================] - 16s 26ms/step - loss: 1.0214 - acc: 0.5214 - val_loss: 1.6284 - val_acc: 0.3824\n",
      "Epoch 84/160\n",
      "608/608 [==============================] - 16s 26ms/step - loss: 1.0152 - acc: 0.5576 - val_loss: 1.6262 - val_acc: 0.3971\n",
      "Epoch 85/160\n",
      "608/608 [==============================] - 16s 26ms/step - loss: 0.9834 - acc: 0.5707 - val_loss: 1.6244 - val_acc: 0.4118\n",
      "Epoch 86/160\n",
      "608/608 [==============================] - 16s 26ms/step - loss: 0.9674 - acc: 0.5806 - val_loss: 1.6257 - val_acc: 0.4118\n",
      "Epoch 87/160\n",
      "608/608 [==============================] - 16s 26ms/step - loss: 1.0125 - acc: 0.5641 - val_loss: 1.6279 - val_acc: 0.4118\n",
      "Epoch 88/160\n",
      "608/608 [==============================] - 16s 27ms/step - loss: 0.9860 - acc: 0.5740 - val_loss: 1.6351 - val_acc: 0.4265\n",
      "Epoch 89/160\n",
      "608/608 [==============================] - 16s 26ms/step - loss: 0.9929 - acc: 0.5559 - val_loss: 1.6456 - val_acc: 0.4265\n",
      "Epoch 90/160\n",
      "608/608 [==============================] - 13s 21ms/step - loss: 1.0110 - acc: 0.5559 - val_loss: 1.6439 - val_acc: 0.4265\n",
      "Epoch 91/160\n",
      "608/608 [==============================] - 15s 25ms/step - loss: 0.9914 - acc: 0.5789 - val_loss: 1.6476 - val_acc: 0.4265\n",
      "Epoch 92/160\n",
      "608/608 [==============================] - 15s 24ms/step - loss: 0.9679 - acc: 0.5724 - val_loss: 1.6469 - val_acc: 0.4265\n",
      "Epoch 93/160\n",
      "608/608 [==============================] - 15s 25ms/step - loss: 0.9946 - acc: 0.5576 - val_loss: 1.6380 - val_acc: 0.4265\n",
      "Epoch 94/160\n",
      "608/608 [==============================] - 18s 30ms/step - loss: 1.0139 - acc: 0.5543 - val_loss: 1.6287 - val_acc: 0.4118\n",
      "Epoch 95/160\n",
      "608/608 [==============================] - 20s 33ms/step - loss: 0.9674 - acc: 0.5773 - val_loss: 1.6131 - val_acc: 0.4118\n",
      "Epoch 96/160\n",
      "608/608 [==============================] - 17s 27ms/step - loss: 0.9694 - acc: 0.5839 - val_loss: 1.6226 - val_acc: 0.4118\n",
      "Epoch 97/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 1.0185 - acc: 0.5477 - val_loss: 1.6247 - val_acc: 0.4118\n",
      "Epoch 98/160\n",
      "608/608 [==============================] - 15s 25ms/step - loss: 0.9965 - acc: 0.5674 - val_loss: 1.6337 - val_acc: 0.4265\n",
      "Epoch 99/160\n",
      "608/608 [==============================] - 15s 25ms/step - loss: 0.9867 - acc: 0.5724 - val_loss: 1.6489 - val_acc: 0.4265\n",
      "Epoch 100/160\n",
      "608/608 [==============================] - 15s 25ms/step - loss: 1.0190 - acc: 0.5510 - val_loss: 1.6298 - val_acc: 0.4265\n",
      "Epoch 101/160\n",
      "608/608 [==============================] - 16s 26ms/step - loss: 0.9856 - acc: 0.5806 - val_loss: 1.6337 - val_acc: 0.4118\n",
      "Epoch 102/160\n",
      "608/608 [==============================] - 15s 24ms/step - loss: 0.9795 - acc: 0.5806 - val_loss: 1.6251 - val_acc: 0.4265\n",
      "Epoch 103/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 0.9713 - acc: 0.5839 - val_loss: 1.6263 - val_acc: 0.4265\n",
      "Epoch 104/160\n",
      "608/608 [==============================] - 16s 26ms/step - loss: 0.9514 - acc: 0.5757 - val_loss: 1.6350 - val_acc: 0.4265\n",
      "Epoch 105/160\n",
      "608/608 [==============================] - 17s 28ms/step - loss: 1.0049 - acc: 0.5789 - val_loss: 1.6413 - val_acc: 0.4265\n",
      "Epoch 106/160\n",
      "608/608 [==============================] - 16s 27ms/step - loss: 0.9817 - acc: 0.5822 - val_loss: 1.6507 - val_acc: 0.4265\n",
      "Epoch 107/160\n",
      "608/608 [==============================] - 17s 28ms/step - loss: 0.9840 - acc: 0.5872 - val_loss: 1.6547 - val_acc: 0.4265\n",
      "Epoch 108/160\n",
      "608/608 [==============================] - 17s 29ms/step - loss: 0.9714 - acc: 0.5789 - val_loss: 1.6628 - val_acc: 0.4265\n",
      "Epoch 109/160\n",
      "608/608 [==============================] - 15s 25ms/step - loss: 1.0013 - acc: 0.5543 - val_loss: 1.6724 - val_acc: 0.4265\n",
      "Epoch 110/160\n",
      "608/608 [==============================] - 14s 22ms/step - loss: 1.0007 - acc: 0.5411 - val_loss: 1.6714 - val_acc: 0.4412\n",
      "Epoch 111/160\n",
      "608/608 [==============================] - 14s 22ms/step - loss: 0.9655 - acc: 0.5740 - val_loss: 1.6675 - val_acc: 0.4265\n",
      "Epoch 112/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 0.9734 - acc: 0.5724 - val_loss: 1.6586 - val_acc: 0.4265\n",
      "Epoch 113/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 0.9632 - acc: 0.5822 - val_loss: 1.6519 - val_acc: 0.4265\n",
      "Epoch 114/160\n",
      "608/608 [==============================] - 15s 25ms/step - loss: 1.0159 - acc: 0.5592 - val_loss: 1.6510 - val_acc: 0.4265\n",
      "Epoch 115/160\n",
      "608/608 [==============================] - 17s 28ms/step - loss: 0.9940 - acc: 0.5576 - val_loss: 1.6432 - val_acc: 0.4265\n",
      "Epoch 116/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 0.9606 - acc: 0.5757 - val_loss: 1.6439 - val_acc: 0.4265\n",
      "Epoch 117/160\n",
      "608/608 [==============================] - 13s 22ms/step - loss: 1.0084 - acc: 0.5247 - val_loss: 1.6599 - val_acc: 0.4265\n",
      "Epoch 118/160\n",
      "608/608 [==============================] - 13s 22ms/step - loss: 0.9835 - acc: 0.5757 - val_loss: 1.6691 - val_acc: 0.4265\n",
      "Epoch 119/160\n",
      "608/608 [==============================] - 13s 22ms/step - loss: 0.9507 - acc: 0.5954 - val_loss: 1.6653 - val_acc: 0.4265\n",
      "Epoch 120/160\n",
      "608/608 [==============================] - 18s 30ms/step - loss: 1.0083 - acc: 0.5296 - val_loss: 1.6663 - val_acc: 0.4412\n",
      "Epoch 121/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "608/608 [==============================] - 17s 27ms/step - loss: 0.9672 - acc: 0.5674 - val_loss: 1.6624 - val_acc: 0.4412\n",
      "Epoch 122/160\n",
      "608/608 [==============================] - 17s 28ms/step - loss: 0.9927 - acc: 0.5789 - val_loss: 1.6536 - val_acc: 0.4265\n",
      "Epoch 123/160\n",
      "608/608 [==============================] - 15s 25ms/step - loss: 0.9712 - acc: 0.5954 - val_loss: 1.6509 - val_acc: 0.3971\n",
      "Epoch 124/160\n",
      "608/608 [==============================] - 15s 25ms/step - loss: 0.9757 - acc: 0.5609 - val_loss: 1.6465 - val_acc: 0.3971\n",
      "Epoch 125/160\n",
      "608/608 [==============================] - 18s 30ms/step - loss: 0.9946 - acc: 0.5954 - val_loss: 1.6430 - val_acc: 0.4118\n",
      "Epoch 126/160\n",
      "608/608 [==============================] - 16s 27ms/step - loss: 0.9798 - acc: 0.5658 - val_loss: 1.6378 - val_acc: 0.4118\n",
      "Epoch 127/160\n",
      "608/608 [==============================] - 13s 22ms/step - loss: 0.9727 - acc: 0.5559 - val_loss: 1.6410 - val_acc: 0.3971\n",
      "Epoch 128/160\n",
      "608/608 [==============================] - 17s 28ms/step - loss: 0.9978 - acc: 0.5658 - val_loss: 1.6354 - val_acc: 0.3971\n",
      "Epoch 129/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 0.9775 - acc: 0.5822 - val_loss: 1.6430 - val_acc: 0.4118\n",
      "Epoch 130/160\n",
      "608/608 [==============================] - 13s 21ms/step - loss: 0.9757 - acc: 0.5691 - val_loss: 1.6446 - val_acc: 0.4118\n",
      "Epoch 131/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 0.9609 - acc: 0.5609 - val_loss: 1.6461 - val_acc: 0.4118\n",
      "Epoch 132/160\n",
      "608/608 [==============================] - 13s 22ms/step - loss: 0.9644 - acc: 0.5888 - val_loss: 1.6533 - val_acc: 0.4265\n",
      "Epoch 133/160\n",
      "608/608 [==============================] - 13s 22ms/step - loss: 0.9734 - acc: 0.6053 - val_loss: 1.6420 - val_acc: 0.4118\n",
      "Epoch 134/160\n",
      "608/608 [==============================] - 13s 22ms/step - loss: 0.9799 - acc: 0.5658 - val_loss: 1.6416 - val_acc: 0.4265\n",
      "Epoch 135/160\n",
      "608/608 [==============================] - 13s 22ms/step - loss: 1.0031 - acc: 0.5559 - val_loss: 1.6405 - val_acc: 0.4118\n",
      "Epoch 136/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 0.9784 - acc: 0.5641 - val_loss: 1.6415 - val_acc: 0.4118\n",
      "Epoch 137/160\n",
      "608/608 [==============================] - 13s 22ms/step - loss: 0.9579 - acc: 0.5691 - val_loss: 1.6461 - val_acc: 0.3971\n",
      "Epoch 138/160\n",
      "608/608 [==============================] - 13s 22ms/step - loss: 0.9671 - acc: 0.5724 - val_loss: 1.6636 - val_acc: 0.4118\n",
      "Epoch 139/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 0.9620 - acc: 0.5658 - val_loss: 1.6767 - val_acc: 0.4265\n",
      "Epoch 140/160\n",
      "608/608 [==============================] - 14s 23ms/step - loss: 0.9748 - acc: 0.5625 - val_loss: 1.6749 - val_acc: 0.4118\n",
      "Epoch 141/160\n",
      "300/608 [=============>................] - ETA: 6s - loss: 1.0066 - acc: 0.5767"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-49aa4fb16607>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mperiod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mchange_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLearningRateScheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m160\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchange_lr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkpoint_path = './weights/weights_best_6.h5'\n",
    "checkpoint = ModelCheckpoint(checkpoint_path,monitor='val_loss',verbose=0,period=1,save_best_only=True)\n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "history = seq_m.fit(fit_features,train_labels, nb_epoch=160, batch_size=20, verbose=1,validation_data=(predict_features,test_labels), callbacks=[checkpoint,change_lr])\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(608, 200, 71)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_features.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
